{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e02ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'a', 'id': 20037, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': 'ad', 'id': 3311, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<unknown>', 'id': 4, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 's', 'id': 20064, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': 'fas', 'id': 6977, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'df', 'id': 20573, 'type': <TokenType.BPE: 'BPE'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ali', 'id': 2697, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ata', 'id': 2212, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'bak', 'id': 2794, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<', 'id': 32086, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'start', 'id': 2418, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '_', 'id': 31905, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'of', 'id': 19524, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '_', 'id': 31905, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'ima', 'id': 3765, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ge', 'id': 20069, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': '>', 'id': 32112, 'type': <TokenType.BPE: 'BPE'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'başka', 'id': 2616, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'bir', 'id': 2501, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'şey', 'id': 2547, 'type': <TokenType.ROOT: 'ROOT'>}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from tr_tokenizer import TRTokenizer\n",
    "\n",
    "tokenizer = TRTokenizer()\n",
    "\n",
    "tokens, upper_idx = tokenizer.tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e420db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aad▁sfasdfaliatabak'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"sıcak\" üçlü, yumuşama ve ünsüz düşmesi\n",
    "# \"bedir\" ünlü düşmesi\n",
    "# \"et\" yumuşama\n",
    "# \"açımla\" kalın genişleme\n",
    "# \"garipse\" ince genişleme\n",
    "# \"hade\" yanlış eklenmiş"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95354ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e24433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211\n",
      "10\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "13\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "roots_with_2_options = []\n",
    "roots_with_3_options = []\n",
    "roots_with_4_options = []\n",
    "roots_with_more_options = []\n",
    "\n",
    "for i in range(20000):\n",
    "  tokens = tokenizer.reverse_dict[i]\n",
    "  if len(tokens) == 2:\n",
    "    roots_with_2_options.append(tokens)\n",
    "  elif len(tokens) == 3:\n",
    "    roots_with_3_options.append(tokens)\n",
    "  elif len(tokens) == 4:\n",
    "    roots_with_4_options.append(tokens)\n",
    "  elif len(tokens) > 4:\n",
    "    roots_with_more_options.append(tokens)\n",
    "\n",
    "suffixes_with_2_options = []\n",
    "suffixes_with_3_options = []\n",
    "suffixes_with_4_options = []\n",
    "suffixes_with_more_options = []\n",
    "\n",
    "for i in range(20000, 30000):\n",
    "  tokens = tokenizer.reverse_dict[i]\n",
    "  if len(tokens) == 2:\n",
    "    suffixes_with_2_options.append(tokens)\n",
    "  elif len(tokens) == 3:\n",
    "    suffixes_with_3_options.append(tokens)\n",
    "  elif len(tokens) == 4:\n",
    "    suffixes_with_4_options.append(tokens)\n",
    "  elif len(tokens) > 4:\n",
    "    suffixes_with_more_options.append(tokens)\n",
    "\n",
    "print(len(roots_with_2_options))\n",
    "print(len(roots_with_3_options))\n",
    "print(len(roots_with_4_options))\n",
    "print(len(roots_with_more_options))\n",
    "print(len(suffixes_with_2_options))\n",
    "print(len(suffixes_with_3_options))\n",
    "print(len(suffixes_with_4_options))\n",
    "print(len(suffixes_with_more_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53c2675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nın', 'nin', 'nun', 'nün', 'yacak'],\n",
       " ['dı', 'di', 'du', 'dü', 'tı', 'ti', 'tu', 'tü'],\n",
       " ['cı', 'ci', 'cu', 'cü', 'çı', 'çi', 'çu', 'çü'],\n",
       " ['dır', 'dir', 'dur', 'dür', 'tır', 'tir', 'tur', 'tür'],\n",
       " ['lık', 'lik', 'luk', 'lük', 'lığ', 'liğ', 'luğ', 'lüğ'],\n",
       " ['cık',\n",
       "  'cik',\n",
       "  'cuk',\n",
       "  'cük',\n",
       "  'çık',\n",
       "  'çik',\n",
       "  'çuk',\n",
       "  'çük',\n",
       "  'cığ',\n",
       "  'ciğ',\n",
       "  'cuğ',\n",
       "  'cüğ',\n",
       "  'çığ',\n",
       "  'çiğ',\n",
       "  'çuğ',\n",
       "  'çüğ'],\n",
       " ['acak', 'ecek', 'acağ', 'eceğ', 'yecek', 'yacağ', 'yeceğ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes_with_more_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0049c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43455bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m ids = tokenizer.encode(\u001b[33m\"\u001b[39m\u001b[33mAli Ata Bak▁ ler lar de da te ta\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/turkish_tokenizer/tr_tokenizer.py:356\u001b[39m, in \u001b[36mTRTokenizer.decode\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m    354\u001b[39m         text += \u001b[38;5;28mself\u001b[39m._select_correct_root(i, ids)\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# token is a suffix since bpe tokens are single tokens\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         text += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_correct_suffix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     text += tokens[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/turkish_tokenizer/tr_tokenizer.py:163\u001b[39m, in \u001b[36mTRTokenizer._select_correct_suffix\u001b[39m\u001b[34m(self, i, ids)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (prev_token == \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m prev_token == \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m prev_token == \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m i > \u001b[32m1\u001b[39m:\n\u001b[32m    162\u001b[39m     prev_token = \u001b[38;5;28mself\u001b[39m.reverse_dict[ids[i - \u001b[32m2\u001b[39m]][\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ends_with_ince\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_token\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m suffixes[\u001b[32m1\u001b[39m]\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m suffixes[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/turkish_tokenizer/tr_tokenizer.py:143\u001b[39m, in \u001b[36mTRTokenizer._ends_with_ince\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ends_with_ince\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ends_with_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meiöü\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/turkish_tokenizer/tr_tokenizer.py:140\u001b[39m, in \u001b[36mTRTokenizer._ends_with_any\u001b[39m\u001b[34m(word, charset, k)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ends_with_any\u001b[39m(word: \u001b[38;5;28mstr\u001b[39m, charset: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Safely check last k chars; slicing handles short words gracefully\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(c \u001b[38;5;129;01min\u001b[39;00m charset \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m word[\u001b[43m-\u001b[49m\u001b[43mk\u001b[49m:]) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: bad operand type for unary -: 'str'"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"Ali Ata Bak▁ ler lar de da te ta\")\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d677259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2e457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"sıcak\" üçlü, yumuşama ve ünsüz düşmesi\n",
    "# \"bedir\" ünlü düşmesi\n",
    "# \"et\" yumuşama\n",
    "# \"açımla\" kalın genişleme\n",
    "# \"garipse\" ince genişleme\n",
    "# \"hade\" yanlış eklenmiş\n",
    "indexler = {\n",
    "  \"üçlü, yumuşama ve ünsüz düşmesi\": 0,\n",
    "  \"ünlü-düşme\": 0,\n",
    "  \"yumuşama\": 0,\n",
    "  \"kalın-genişleme\": 0,\n",
    "  \"ince-genişleme\": 0,\n",
    "  \"yanlış eklenmiş\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00033e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"v09_kokler.json\", \"r\") as f:\n",
    "    v09_kokler = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4f56d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'üçlü, yumuşama ve ünsüz düşmesi': 100,\n",
       "  'ünlü-düşme': 110,\n",
       "  'yumuşama': 198,\n",
       "  'kalın-genişleme': 2080,\n",
       "  'ince-genişleme': 2223,\n",
       "  'yanlış eklenmiş': 2315},\n",
       " 20000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v09_kokler = {}\n",
    "\n",
    "i = 0\n",
    "current_id = 0\n",
    "\n",
    "for key, value in v09_kokler.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_v09_kokler[key] = i\n",
    "  if key == \"sıcak\":\n",
    "    indexler[\"üçlü, yumuşama ve ünsüz düşmesi\"] = i\n",
    "  elif key == \"bedir\":\n",
    "    indexler[\"ünlü-düşme\"] = i\n",
    "  elif key == \"et\":\n",
    "    indexler[\"yumuşama\"] = i\n",
    "  elif key == \"açımla\":\n",
    "    indexler[\"kalın-genişleme\"] = i\n",
    "  elif key == \"garipse\":\n",
    "    indexler[\"ince-genişleme\"] = i\n",
    "  elif key == \"hade\":\n",
    "    indexler[\"yanlış eklenmiş\"] = i\n",
    "\n",
    "i += 1\n",
    "while i < 20000:\n",
    "  new_v09_kokler[f\"kok_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_kokler_new.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_v09_kokler, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "indexler, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "479b6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c78c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bpe_v05.json\", \"r\") as f:\n",
    "    bpe_tokens = json.load(f)\n",
    "\n",
    "with open(\"ekler_v05.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "with open(\"kokler_v07.json\", \"r\") as f:\n",
    "    roots = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c49b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20256\n"
     ]
    }
   ],
   "source": [
    "new_suffixes = {}\n",
    "i -= 1\n",
    "for key, value in suffixes.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_suffixes[key] = i\n",
    "\n",
    "i += 1\n",
    "\n",
    "while i < 20256:\n",
    "  new_suffixes[f\"ek_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_ekler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_suffixes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d35113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeteations = []\n",
    "\n",
    "for key in suffixes.keys():\n",
    "  if key in roots:\n",
    "    repeteations.append(key)\n",
    "    del roots[key]\n",
    "\n",
    "len(repeteations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4b2326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_roots = {}\n",
    "\n",
    "root_keys = list(roots.keys())\n",
    "for i in range(100):\n",
    "  new_roots[root_keys[i]] = i\n",
    "  del roots[root_keys[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bde7b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(roots_with_2_options):\n",
    "  for new_root in roots_with_2_options[i]:\n",
    "    new_roots[new_root] = i + 100\n",
    "    del roots[new_root]\n",
    "  i += 1\n",
    "i = i + 100\n",
    "k = i\n",
    "\n",
    "j = 0\n",
    "while i < k + len(roots_with_3_options):\n",
    "  for new_root in roots_with_3_options[j]:\n",
    "    new_roots[new_root] = i\n",
    "    del roots[new_root]\n",
    "  i += 1\n",
    "  j += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "161ccd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 çüş 20032\n",
      "19529\n",
      "20000\n",
      "20256\n"
     ]
    }
   ],
   "source": [
    "current_id = 0\n",
    "i = 2502\n",
    "for key, value in roots.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_roots[key] = i\n",
    "print(i, key, value)\n",
    "print(i)\n",
    "while i < 20000:\n",
    "  i = i + 1\n",
    "  new_roots[f\"kok_temp_{i}\"] = i\n",
    "\n",
    "print(i)\n",
    "\n",
    "with open(\"v09_kokler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_roots, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "new_suffixes = {}\n",
    "for key, value in suffixes.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_suffixes[key] = i\n",
    "\n",
    "i += 1\n",
    "\n",
    "while i < 20256:\n",
    "  new_suffixes[f\"ek_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_ekler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_suffixes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3715bc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938fbef4434a43708ed2bfe3434b8e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00004-of-00010.parquet:   0%|          | 0.00/224M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e527879372948a9acdaa7cdc0a56be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00005-of-00010.parquet:   0%|          | 0.00/237M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25fd4e7649347f0ae4e39afb4bd0bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00006-of-00010.parquet:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720d6da4f0c419eb8d8800de996e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00007-of-00010.parquet:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5e779748134c77ae9bf9a6ce4c5c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00008-of-00010.parquet:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd12c1f24b34dba83151ee4c342a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00009-of-00010.parquet:   0%|          | 0.00/149M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ce957054c43f98a13f63a0ff1f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'raw_mediawiki', 'text'],\n",
       "    num_rows: 641443\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English dataset from the latest dump\n",
    "ds_wiki = load_dataset(\"omarkamali/wikipedia-monthly\", \"20250703.tr\", split=\"train\")\n",
    "ds_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28b4511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0c57efdafe40acb0d2552747047e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6445abbec73e41bfad0c35c3b177878c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91818713cc14e21ade101810661b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2458c413264da78d70fe39f84fc791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2563449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Puan', 'Tarih', 'Baslik', 'Yorum', 'Konum', 'Yoruma verilen artı sayısı', 'Yoruma verilen eksi sayısı'],\n",
       "        num_rows: 2563449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_yorumbudur = load_dataset(\"alibayram/yorumbudur\")\n",
    "ds_yorumbudur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24dc1229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182fe0c34cb4095835d501b8c818369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09f7437662d4cc091a6ba1f1d2ec86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hepsiburadaYorumlar.parquet:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87095dcdb57a4590a98b28de4b975d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2657073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Puan', 'Baslik', 'Yorum'],\n",
       "        num_rows: 2657073\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_hepsiburada = load_dataset(\"alibayram/hepsiburada_yorumlar\")\n",
    "ds_hepsiburada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73712f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_wiki 0\n",
      "ds_wiki 50000\n",
      "ds_wiki 100000\n",
      "ds_wiki 150000\n",
      "ds_wiki 200000\n",
      "ds_wiki 250000\n",
      "ds_wiki 300000\n",
      "ds_wiki 350000\n",
      "ds_wiki 400000\n",
      "ds_wiki 450000\n",
      "ds_wiki 500000\n",
      "ds_wiki 550000\n",
      "ds_wiki 600000\n",
      "641443\n",
      "ds_yorumbudur 0\n",
      "ds_yorumbudur 50000\n",
      "ds_yorumbudur 100000\n",
      "ds_yorumbudur 150000\n",
      "ds_yorumbudur 200000\n",
      "ds_yorumbudur 250000\n",
      "ds_yorumbudur 300000\n",
      "ds_yorumbudur 350000\n",
      "ds_yorumbudur 400000\n",
      "ds_yorumbudur 450000\n",
      "ds_yorumbudur 500000\n",
      "ds_yorumbudur 550000\n",
      "ds_yorumbudur 600000\n",
      "ds_yorumbudur 650000\n",
      "ds_yorumbudur 700000\n",
      "ds_yorumbudur 750000\n",
      "ds_yorumbudur 800000\n",
      "ds_yorumbudur 850000\n",
      "ds_yorumbudur 900000\n",
      "ds_yorumbudur 950000\n",
      "ds_yorumbudur 1000000\n",
      "ds_yorumbudur 1050000\n",
      "ds_yorumbudur 1100000\n",
      "ds_yorumbudur 1150000\n",
      "ds_yorumbudur 1200000\n",
      "ds_yorumbudur 1250000\n",
      "ds_yorumbudur 1300000\n",
      "ds_yorumbudur 1350000\n",
      "ds_yorumbudur 1400000\n",
      "ds_yorumbudur 1450000\n",
      "ds_yorumbudur 1500000\n",
      "ds_yorumbudur 1550000\n",
      "ds_yorumbudur 1600000\n",
      "ds_yorumbudur 1650000\n",
      "ds_yorumbudur 1700000\n",
      "ds_yorumbudur 1750000\n",
      "ds_yorumbudur 1800000\n",
      "ds_yorumbudur 1850000\n",
      "ds_yorumbudur 1900000\n",
      "ds_yorumbudur 1950000\n",
      "ds_yorumbudur 2000000\n",
      "ds_yorumbudur 2050000\n",
      "ds_yorumbudur 2100000\n",
      "ds_yorumbudur 2150000\n",
      "ds_yorumbudur 2200000\n",
      "ds_yorumbudur 2250000\n",
      "ds_yorumbudur 2300000\n",
      "ds_yorumbudur 2350000\n",
      "ds_yorumbudur 2400000\n",
      "ds_yorumbudur 2450000\n",
      "ds_yorumbudur 2500000\n",
      "ds_yorumbudur 2550000\n",
      "3204892\n",
      "ds_hepsiburada 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_hepsiburada[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m   text = \u001b[43mds_hepsiburada\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYorum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     16\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (i % \u001b[32m50000\u001b[39m == \u001b[32m0\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mds_hepsiburada\u001b[39m\u001b[33m\"\u001b[39m, i)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for i in range(len(ds_wiki)):\n",
    "  text = ds_wiki[i]['text'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_wiki\", i)\n",
    "  texts.append(text)\n",
    "print(len(texts))\n",
    "for i in range(len(ds_yorumbudur['train'])):\n",
    "  text = ds_yorumbudur['train'][i]['Yorum'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_yorumbudur\", i)\n",
    "  texts.append(text)\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c75c9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_hepsiburada 0\n",
      "ds_hepsiburada 50000\n",
      "ds_hepsiburada 100000\n",
      "ds_hepsiburada 150000\n",
      "ds_hepsiburada 200000\n",
      "ds_hepsiburada 250000\n",
      "ds_hepsiburada 300000\n",
      "ds_hepsiburada 350000\n",
      "ds_hepsiburada 400000\n",
      "ds_hepsiburada 450000\n",
      "ds_hepsiburada 500000\n",
      "ds_hepsiburada 550000\n",
      "ds_hepsiburada 600000\n",
      "ds_hepsiburada 650000\n",
      "ds_hepsiburada 700000\n",
      "ds_hepsiburada 750000\n",
      "ds_hepsiburada 800000\n",
      "ds_hepsiburada 850000\n",
      "ds_hepsiburada 900000\n",
      "ds_hepsiburada 950000\n",
      "ds_hepsiburada 1000000\n",
      "ds_hepsiburada 1050000\n",
      "ds_hepsiburada 1100000\n",
      "ds_hepsiburada 1150000\n",
      "ds_hepsiburada 1200000\n",
      "ds_hepsiburada 1250000\n",
      "ds_hepsiburada 1300000\n",
      "ds_hepsiburada 1350000\n",
      "ds_hepsiburada 1400000\n",
      "ds_hepsiburada 1450000\n",
      "ds_hepsiburada 1500000\n",
      "ds_hepsiburada 1550000\n",
      "ds_hepsiburada 1600000\n",
      "ds_hepsiburada 1650000\n",
      "ds_hepsiburada 1700000\n",
      "ds_hepsiburada 1750000\n",
      "ds_hepsiburada 1800000\n",
      "ds_hepsiburada 1850000\n",
      "ds_hepsiburada 1900000\n",
      "ds_hepsiburada 1950000\n",
      "ds_hepsiburada 2000000\n",
      "ds_hepsiburada 2050000\n",
      "ds_hepsiburada 2100000\n",
      "ds_hepsiburada 2150000\n",
      "ds_hepsiburada 2200000\n",
      "ds_hepsiburada 2250000\n",
      "ds_hepsiburada 2300000\n",
      "ds_hepsiburada 2350000\n",
      "ds_hepsiburada 2400000\n",
      "ds_hepsiburada 2450000\n",
      "ds_hepsiburada 2500000\n",
      "ds_hepsiburada 2550000\n",
      "ds_hepsiburada 2600000\n",
      "ds_hepsiburada 2650000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5879363"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(ds_hepsiburada['train'])):\n",
    "  if ds_hepsiburada['train'][i]['Yorum'] == None:\n",
    "    continue\n",
    "  text = ds_hepsiburada['train'][i]['Yorum'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_hepsiburada\", i)\n",
    "  texts.append(text)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f1d674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"film şu anlamlara gelebilir:\\n\\n camlara yapıştırılarak içerinin görünmesini engelleyen bir tür ince yaprak\\n sinemacılıkta, bir oyunun bütününü taşıyan şerit veya şeritlerin bütünü\\n film (fotoğrafçılık), fotoğrafçılıkta, radyografide ve sinemacılıkta resim çekmek için kullanılan; selülozdan, saydam, bükülebilir şerit\\n film (sinema), sinema makinesiyle gösterilen eser, izleti\\n film (film), samuel beckett'in yazdığı tek senaryodan çekilen 1965 abd yapımı film\\n total film, 1997'den beri i̇ngiltere'de yayımlanan sinema dergisi\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2387f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.add_tokens(list(new_roots.keys()))\n",
    "tokenizer.add_tokens(list(new_suffixes.keys()))\n",
    "trainer = BpeTrainer(vocab_size=40000)\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "with open(\"v09_bpe_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(tokenizer.get_vocab(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "368df881",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tv09_raw_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efee8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokens = {}\n",
    "i = 20256\n",
    "\n",
    "tokenizer_vocab = tokenizer.get_vocab().keys()\n",
    "\n",
    "# filter keys that shorter than 2 characters\n",
    "tokenizer_vocab = [key for key in tokenizer_vocab if len(key) > 1]\n",
    "\n",
    "# sort tokenizer_vocab by length of the key it is string short to long\n",
    "tokenizer_vocab = sorted(tokenizer_vocab, key=len)\n",
    "\n",
    "for key in tokenizer_vocab:\n",
    "  if key in new_suffixes.keys():\n",
    "    continue\n",
    "  if key in new_roots.keys():\n",
    "    continue\n",
    "  bpe_tokens[key] = i\n",
    "  i += 1\n",
    "  if i >= 31768:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9116a141f8454dbcc4b378c31168f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e7fff5ef68418585ebac8548319213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a39be6985f493e837403aaab0ace38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/267M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9d226daf3c49f8945a38cf7874f935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/317423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "317423"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds_haber = load_dataset(\"habanoz/24-news-tr-v1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(len(ds_haber['train'])):\n",
    "  text = ds_haber['train'][i]['Text'].lower()\n",
    "  texts.append(text)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e77aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m new_roots.keys():\n\u001b[32m     21\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer_vocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m():\n\u001b[32m     23\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     24\u001b[39m bpe_tokens[key] = i\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "news_tokenizer = Tokenizer(BPE())\n",
    "news_tokenizer.pre_tokenizer = Whitespace()\n",
    "news_tokenizer.add_tokens(list(new_roots.keys()))\n",
    "news_tokenizer.add_tokens(list(new_suffixes.keys()))\n",
    "news_trainer = BpeTrainer(vocab_size=40000)\n",
    "\n",
    "\n",
    "news_tokenizer.train_from_iterator(texts, trainer=news_trainer)\n",
    "\n",
    "bpe_tokens = {}\n",
    "\n",
    "news_tokenizer_vocab = news_tokenizer.get_vocab().keys()\n",
    "\n",
    "# sort tokenizer_vocab by length of the key it is string short to long\n",
    "news_tokenizer_vocab = sorted(news_tokenizer_vocab, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf81b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in news_tokenizer_vocab:\n",
    "  if key in new_suffixes.keys():\n",
    "    continue\n",
    "  if key in new_roots.keys():\n",
    "    continue\n",
    "  if key in tokenizer_vocab:\n",
    "    continue\n",
    "  bpe_tokens[key] = i\n",
    "  i += 1\n",
    "  if i >= 32768:\n",
    "    break\n",
    "\n",
    "\n",
    "with open(\"v09_bpe_tokens.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(bpe_tokens, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c81bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['a', 'a']\n",
      "['a', 'a', 'd']\n",
      "['a', 'a', 'd', 'sf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([22274, 22274, 32266, 20568], ['a', 'a', 'd', 'sf'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"aadsfasdfAliAtaBak\"\n",
    "uppercase_indices = [i for i, c in enumerate(word) if c.isupper()]\n",
    "word = word.lower()\n",
    "\n",
    "# check if word is a root if not, recursively drop the last letter until it is a root\n",
    "# or it is 2 letters then check if it is a suffix recursively by dropping the last letter\n",
    "# if the remaining part is not root or suffix then check if it is a bpe token\n",
    "# and remove that part from the word and continue with the remaining part\n",
    "\n",
    "tokens = []\n",
    "ids = []\n",
    "\n",
    "def get_longest_root_id(word: str) -> tuple[int, str]:\n",
    "  if word in roots:\n",
    "    return roots[word], word\n",
    "  if len(word) <= 2:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_longest_root_id(word[:-1])\n",
    "\n",
    "id, root = get_longest_root_id(word)\n",
    "\n",
    "while len(word) > 2 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(root)\n",
    "  word = word[len(root):]\n",
    "  print(tokens)\n",
    "  id, root = get_longest_root_id(word)\n",
    "\n",
    "def get_longest_suffix_id(word: str) -> tuple[int, str]:\n",
    "  if word in suffixes:\n",
    "    return suffixes[word], word\n",
    "  if len(word) <= 1:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_longest_suffix_id(word[:-1])\n",
    "\n",
    "id, suffix = get_longest_suffix_id(word)\n",
    "\n",
    "while len(word) > 1 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(suffix)\n",
    "  word = word[len(suffix):]\n",
    "  print(tokens)\n",
    "  id, suffix = get_longest_suffix_id(word)\n",
    "\n",
    "def get_bpe_id(word: str) -> tuple[int, str]:\n",
    "  if word in bpe_tokens:\n",
    "    return bpe_tokens[word], word\n",
    "  if len(word) <= 1:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_bpe_id(word[:-1])\n",
    "\n",
    "id, bpe = get_bpe_id(word)\n",
    "\n",
    "while len(word) > 0 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(bpe)\n",
    "  word = word[len(bpe):]\n",
    "  print(tokens)\n",
    "  id, bpe = get_bpe_id(word)\n",
    "\n",
    "ids, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "339cf924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _uppercase_indices(text: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start in the text.\"\"\"\n",
    "    return [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "def tokenize_word(s: str) -> List[dict]:\n",
    "    \"\"\"Tokenize a single word without camel splitting.\"\"\"\n",
    "    result: List[dict] = []\n",
    "\n",
    "    while s:\n",
    "        # 1) Try ROOT\n",
    "        rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "        if rid is not None:\n",
    "            result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "            s = s[len(rtok):]\n",
    "            continue\n",
    "\n",
    "        # 2) Try SUFFIX\n",
    "        sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "        if sid is not None:\n",
    "            result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "            s = s[len(stok):]\n",
    "            continue\n",
    "\n",
    "        # 3) Try BPE\n",
    "        bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "        if bid is not None:\n",
    "            result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "            s = s[len(btok):]\n",
    "            continue\n",
    "\n",
    "        # 4) No match → UNK for one char\n",
    "        result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "        s = s[1:]\n",
    "\n",
    "    return result\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize whole text, splitting from spaces only.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    upper_idx = _uppercase_indices(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for idx, word in enumerate(words):\n",
    "        if word != \"\":\n",
    "            final_tokens.extend(tokenize_word(word))\n",
    "        if idx < len(words) - 1:\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, upper_idx\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7b38600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "        # after each segment put a space token if it is not the last segment\n",
    "        result.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eaa86efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'as', 'id': 1249, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': 'li', 'id': 22324, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}]\n",
      "[11, 12, 14, 21, 24]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "        # after each segment put a space token if it is not the last segment\n",
    "        result.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAAaAasdfliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "410af126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'h', 'id': 32012, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 't', 'id': 32053, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 't', 'id': 32053, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'p', 'id': 31974, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'server', 'id': 3883, 'type': 'ROOT'}, {'token': '.', 'id': 31897, 'type': 'BPE'}, {'token': 'ca', 'id': 1212, 'type': 'ROOT'}, {'token': 'll', 'id': 20489, 'type': 'BPE'}, {'token': '()', 'id': 20803, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}]\n",
      "[11, 14, 17, 20, 21, 22, 23, 24, 44, 47]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    # Track current position in the ORIGINAL word so we know\n",
    "    # whether a segment starts with an uppercase letter.\n",
    "    orig_pos = 0\n",
    "\n",
    "    for seg in _split_camel(word):  # seg is lowercased\n",
    "        # If the current segment starts at an uppercase in the original word,\n",
    "        # emit the <uppercase> marker (id=0) BEFORE tokenizing the segment.\n",
    "        if orig_pos < len(word) and word[orig_pos].isupper():\n",
    "            result.append({\"token\": \"<uppercase>\", \"id\": 0, \"type\": TokenType.ROOT})\n",
    "\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                orig_pos += len(rtok)\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                orig_pos += len(stok)\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                orig_pos += len(btok)\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char (advance orig_pos by 1)\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "            orig_pos += 1\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBakHTTPServer.call() başka BirŞey\")\n",
    "print(tokens)       # includes \"<uppercase>\" before \"ali\", \"bir\", and \"şey\" segments that start uppercase\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0d1deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(tokens, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4277a8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1249, 20573, 1053, 32078, 22268, 32222, 31768, 32072, 32037, 32037, 31832, 31832, 32149, 32037, 22323] ['as', 'df', 'ad', '$', '123', '~', 'q', 'w', 'æ', 'æ', '@', '@', '®', 'æ', 's']\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Example dicts (fill with your real mappings)\n",
    "# roots = {\"ali\": 101, \"ata\": 102, \"bak\": 103, ...}\n",
    "# suffixes = {\"lar\": 201, \"ler\": 202, \"da\": 203, \"de\": 204, ...}\n",
    "# bpe_tokens = {\"aa\": 301, \"ds\": 302, \"fa\": 303, ...}\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    # Check longest to shortest prefix\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Indices where uppercase letters start (CamelCase boundaries).\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    \"\"\"Split word on uppercase indices, keep original order, lowercase parts.\"\"\"\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(\n",
    "    word: str,\n",
    "    roots: Dict[str, int],\n",
    "    suffixes: Dict[str, int],\n",
    "    bpe_tokens: Dict[str, int],\n",
    "    unk_token: str = \"<unknown>\"\n",
    ") -> Tuple[List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Repeats until segment is exhausted; falls back to <UNK> for 1 char if no match.\n",
    "    Handles CamelCase by splitting and tokenizing each segment independently.\n",
    "    \"\"\"\n",
    "    all_ids: List[int] = []\n",
    "    all_tokens: List[str] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            progressed = False\n",
    "\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                all_ids.append(rid)\n",
    "                all_tokens.append(rtok)\n",
    "                s = s[len(rtok):]\n",
    "                progressed = True\n",
    "                continue  # after root, we try root again on the new remainder\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                all_ids.append(sid)\n",
    "                all_tokens.append(stok)\n",
    "                s = s[len(stok):]\n",
    "                progressed = True\n",
    "                # IMPORTANT: after suffix, loop restarts and tries ROOT again first\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                all_ids.append(bid)\n",
    "                all_tokens.append(btok)\n",
    "                s = s[len(btok):]\n",
    "                progressed = True\n",
    "                # IMPORTANT: after BPE, loop restarts and tries ROOT → SUFFIX again\n",
    "                continue\n",
    "\n",
    "            # 4) Nothing matched: emit UNK for one character to avoid stalling\n",
    "            all_tokens.append(unk_token)\n",
    "            all_ids.append(-1)  # or your UNK id\n",
    "            s = s[1:]\n",
    "            progressed = True\n",
    "\n",
    "            if not progressed:\n",
    "                # safety (shouldn't happen due to UNK logic)\n",
    "                break\n",
    "\n",
    "    return all_ids, all_tokens\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "text_with_unkown_token = \"asdfad$123~qwææ@@®æs\"\n",
    "ids, toks = tokenize_word(text_with_unkown_token, roots, suffixes, bpe_tokens)\n",
    "print(ids, toks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrTokenizer:\n",
    "  def __init__(self,\n",
    "               roots: dict[str, int],\n",
    "               suffixes: dict[str, int],\n",
    "               bpe_tokens: dict[str, int]\n",
    "              ):\n",
    "    self.roots = roots\n",
    "    self.suffixes = suffixes\n",
    "    self.bpe_tokens = bpe_tokens\n",
    "\n",
    "  def _get_root_id(self, word: str) -> tuple[int, str]:\n",
    "    if word in self.roots:\n",
    "      return self.roots[word], word\n",
    "    if len(word) == 2:\n",
    "      # not found\n",
    "      return None, word\n",
    "    return self._get_root_id(word[:-1])\n",
    "\n",
    "  def tokenize(self, text: str) -> list[int]:\n",
    "    tokens = []\n",
    "    uppercase_indices = [i for i, c in enumerate(text) if c.isupper()]\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for word in words:\n",
    "      # check if word is a root if not, recursively drop the last letter until it is a root\n",
    "      # or it is 2 letters then check if it is a suffix recursively by dropping the last letter\n",
    "      # if the remaining part is not root or suffix then check if it is a bpe token\n",
    "      # and remove that part from the word and continue with the remaining part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bed4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['er',\n",
       " 'hab',\n",
       " 'a',\n",
       " ',',\n",
       " 'ben',\n",
       " 'li',\n",
       " 'ayr',\n",
       " 'a',\n",
       " 'm',\n",
       " '.',\n",
       " 'en',\n",
       " 'bir',\n",
       " 'yazılım',\n",
       " 'geliştir',\n",
       " 'i',\n",
       " 'ci',\n",
       " 'yi',\n",
       " 'm',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Merhaba, ben Ali Bayram. Ben bir yazılım geliştiriciyim.\"\n",
    "\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "\n",
    "tokens.tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
