{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e02ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'a', 'id': 20037, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': 'ad', 'id': 3311, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<unknown>', 'id': 4, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 's', 'id': 20064, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': 'fas', 'id': 6977, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'df', 'id': 20573, 'type': <TokenType.BPE: 'BPE'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ali', 'id': 2697, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ata', 'id': 2212, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<uppercase>', 'id': 0, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'bak', 'id': 2794, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<', 'id': 32086, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'start', 'id': 2418, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '_', 'id': 31905, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'of', 'id': 19524, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '_', 'id': 31905, 'type': <TokenType.BPE: 'BPE'>}, {'token': 'ima', 'id': 3765, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'ge', 'id': 20069, 'type': <TokenType.SUFFIX: 'SUFFIX'>}, {'token': '>', 'id': 32112, 'type': <TokenType.BPE: 'BPE'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'başka', 'id': 2616, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'bir', 'id': 2501, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': '<space>', 'id': 1, 'type': <TokenType.ROOT: 'ROOT'>}, {'token': 'şey', 'id': 2547, 'type': <TokenType.ROOT: 'ROOT'>}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from tr_tokenizer import TRTokenizer\n",
    "\n",
    "tokenizer = TRTokenizer()\n",
    "\n",
    "tokens, upper_idx = tokenizer.tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e420db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aad▁sfasdfaliatabak'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"sıcak\" üçlü, yumuşama ve ünsüz düşmesi\n",
    "# \"bedir\" ünlü düşmesi\n",
    "# \"et\" yumuşama\n",
    "# \"açımla\" kalın genişleme\n",
    "# \"garipse\" ince genişleme\n",
    "# \"hade\" yanlış eklenmiş"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95354ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e24433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2211\n",
      "10\n",
      "0\n",
      "0\n",
      "13\n",
      "0\n",
      "13\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "roots_with_2_options = []\n",
    "roots_with_3_options = []\n",
    "roots_with_4_options = []\n",
    "roots_with_more_options = []\n",
    "\n",
    "for i in range(20000):\n",
    "  tokens = tokenizer.reverse_dict[i]\n",
    "  if len(tokens) == 2:\n",
    "    roots_with_2_options.append(tokens)\n",
    "  elif len(tokens) == 3:\n",
    "    roots_with_3_options.append(tokens)\n",
    "  elif len(tokens) == 4:\n",
    "    roots_with_4_options.append(tokens)\n",
    "  elif len(tokens) > 4:\n",
    "    roots_with_more_options.append(tokens)\n",
    "\n",
    "suffixes_with_2_options = []\n",
    "suffixes_with_3_options = []\n",
    "suffixes_with_4_options = []\n",
    "suffixes_with_more_options = []\n",
    "\n",
    "for i in range(20000, 30000):\n",
    "  tokens = tokenizer.reverse_dict[i]\n",
    "  if len(tokens) == 2:\n",
    "    suffixes_with_2_options.append(tokens)\n",
    "  elif len(tokens) == 3:\n",
    "    suffixes_with_3_options.append(tokens)\n",
    "  elif len(tokens) == 4:\n",
    "    suffixes_with_4_options.append(tokens)\n",
    "  elif len(tokens) > 4:\n",
    "    suffixes_with_more_options.append(tokens)\n",
    "\n",
    "print(len(roots_with_2_options))\n",
    "print(len(roots_with_3_options))\n",
    "print(len(roots_with_4_options))\n",
    "print(len(roots_with_more_options))\n",
    "print(len(suffixes_with_2_options))\n",
    "print(len(suffixes_with_3_options))\n",
    "print(len(suffixes_with_4_options))\n",
    "print(len(suffixes_with_more_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53c2675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nın', 'nin', 'nun', 'nün', 'yacak'],\n",
       " ['dı', 'di', 'du', 'dü', 'tı', 'ti', 'tu', 'tü'],\n",
       " ['cı', 'ci', 'cu', 'cü', 'çı', 'çi', 'çu', 'çü'],\n",
       " ['dır', 'dir', 'dur', 'dür', 'tır', 'tir', 'tur', 'tür'],\n",
       " ['lık', 'lik', 'luk', 'lük', 'lığ', 'liğ', 'luğ', 'lüğ'],\n",
       " ['cık',\n",
       "  'cik',\n",
       "  'cuk',\n",
       "  'cük',\n",
       "  'çık',\n",
       "  'çik',\n",
       "  'çuk',\n",
       "  'çük',\n",
       "  'cığ',\n",
       "  'ciğ',\n",
       "  'cuğ',\n",
       "  'cüğ',\n",
       "  'çığ',\n",
       "  'çiğ',\n",
       "  'çuğ',\n",
       "  'çüğ'],\n",
       " ['acak', 'ecek', 'acağ', 'eceğ', 'yecek', 'yacağ', 'yeceğ']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes_with_more_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee0049c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 0,\n",
       " 2891,\n",
       " 1,\n",
       " 0,\n",
       " 2697,\n",
       " 31952,\n",
       " 1,\n",
       " 3433,\n",
       " 1,\n",
       " 3347,\n",
       " 1,\n",
       " 2611,\n",
       " 20024,\n",
       " 1,\n",
       " 0,\n",
       " 20033,\n",
       " 32131,\n",
       " 20064,\n",
       " 20025,\n",
       " 2781,\n",
       " 31817,\n",
       " 20021,\n",
       " 1,\n",
       " 3574,\n",
       " 1,\n",
       " 2501,\n",
       " 1,\n",
       " 2797,\n",
       " 20021,\n",
       " 20021,\n",
       " 1,\n",
       " 3677,\n",
       " 10641,\n",
       " 20024,\n",
       " 20059,\n",
       " 1,\n",
       " 2714,\n",
       " 1,\n",
       " 2573,\n",
       " 20021,\n",
       " 20021,\n",
       " 1,\n",
       " 2977,\n",
       " 2189,\n",
       " 1,\n",
       " 3221,\n",
       " 20005,\n",
       " 1,\n",
       " 2661,\n",
       " 31974,\n",
       " 1,\n",
       " 3366,\n",
       " 20001,\n",
       " 1,\n",
       " 2733,\n",
       " 31897,\n",
       " 1,\n",
       " 0,\n",
       " 227,\n",
       " 20025,\n",
       " 1,\n",
       " 3176,\n",
       " 1,\n",
       " 2507,\n",
       " 20026,\n",
       " 16867,\n",
       " 1,\n",
       " 188,\n",
       " 20021,\n",
       " 20033,\n",
       " 1,\n",
       " 4157,\n",
       " 20038,\n",
       " 1,\n",
       " 2675,\n",
       " 20026,\n",
       " 1,\n",
       " 19497,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2503,\n",
       " 1,\n",
       " 2735,\n",
       " 20022,\n",
       " 1,\n",
       " 3028,\n",
       " 20000,\n",
       " 20034,\n",
       " 1,\n",
       " 296,\n",
       " 1,\n",
       " 7238,\n",
       " 1,\n",
       " 2685,\n",
       " 2516,\n",
       " 4926,\n",
       " 31952,\n",
       " 1,\n",
       " 19490,\n",
       " 1,\n",
       " 2995,\n",
       " 1,\n",
       " 2502,\n",
       " 2617,\n",
       " 32267,\n",
       " 1,\n",
       " 2465,\n",
       " 1,\n",
       " 2465,\n",
       " 1,\n",
       " 2766,\n",
       " 20023,\n",
       " 3452,\n",
       " 32267,\n",
       " 31952,\n",
       " 4,\n",
       " 1,\n",
       " 20024,\n",
       " 20026,\n",
       " 31897,\n",
       " 1,\n",
       " 0,\n",
       " 4153,\n",
       " 20001,\n",
       " 1,\n",
       " 19488,\n",
       " 1,\n",
       " 302,\n",
       " 20024,\n",
       " 1,\n",
       " 2701,\n",
       " 1,\n",
       " 2501,\n",
       " 1,\n",
       " 3080,\n",
       " 1,\n",
       " 2464,\n",
       " 20026,\n",
       " 31828,\n",
       " 1,\n",
       " 3820,\n",
       " 20023,\n",
       " 1,\n",
       " 19497,\n",
       " 1,\n",
       " 2643,\n",
       " 20034,\n",
       " 20023,\n",
       " 1,\n",
       " 2787,\n",
       " 1,\n",
       " 2787,\n",
       " 20038,\n",
       " 1,\n",
       " 3110,\n",
       " 20036,\n",
       " 31974,\n",
       " 1,\n",
       " 2935,\n",
       " 9814,\n",
       " 32267,\n",
       " 1,\n",
       " 2313,\n",
       " 4106,\n",
       " 1,\n",
       " 3039,\n",
       " 20018,\n",
       " 3452,\n",
       " 1,\n",
       " 2973,\n",
       " 20026,\n",
       " 31897,\n",
       " 2,\n",
       " 0,\n",
       " 3080,\n",
       " 20025,\n",
       " 1,\n",
       " 19490,\n",
       " 1,\n",
       " 530,\n",
       " 20034,\n",
       " 1,\n",
       " 2573,\n",
       " 20024,\n",
       " 1,\n",
       " 2883,\n",
       " 20023,\n",
       " 20022,\n",
       " 1,\n",
       " 2313,\n",
       " 4106,\n",
       " 1,\n",
       " 2728,\n",
       " 11620,\n",
       " 9814,\n",
       " 32267,\n",
       " 1,\n",
       " 19497,\n",
       " 1,\n",
       " 2464,\n",
       " 20032,\n",
       " 20034,\n",
       " 1,\n",
       " 3377,\n",
       " 20000,\n",
       " 20033,\n",
       " 1,\n",
       " 2852,\n",
       " 20023,\n",
       " 20022,\n",
       " 20026,\n",
       " 31828,\n",
       " 1,\n",
       " 19511,\n",
       " 1,\n",
       " 343,\n",
       " 20025,\n",
       " 1,\n",
       " 2866,\n",
       " 20012,\n",
       " 1,\n",
       " 891,\n",
       " 1,\n",
       " 2501,\n",
       " 1,\n",
       " 3529,\n",
       " 20030,\n",
       " 1,\n",
       " 276,\n",
       " 14023,\n",
       " 1,\n",
       " 3924,\n",
       " 3539,\n",
       " 1,\n",
       " 3366,\n",
       " 20024,\n",
       " 1,\n",
       " 414,\n",
       " 1,\n",
       " 4415,\n",
       " 20000,\n",
       " 20033,\n",
       " 1,\n",
       " 2478,\n",
       " 6132,\n",
       " 20031,\n",
       " 20033,\n",
       " 1,\n",
       " 2469,\n",
       " 20026,\n",
       " 31897,\n",
       " 1,\n",
       " 0,\n",
       " 4163,\n",
       " 20029,\n",
       " 20035,\n",
       " 1,\n",
       " 12040,\n",
       " 3452,\n",
       " 31952,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2503,\n",
       " 1,\n",
       " 189,\n",
       " 20021,\n",
       " 1,\n",
       " 2535,\n",
       " 1,\n",
       " 2664,\n",
       " 20021,\n",
       " 20036,\n",
       " 1,\n",
       " 2701,\n",
       " 20030,\n",
       " 1,\n",
       " 3008,\n",
       " 3649,\n",
       " 20039,\n",
       " 31952,\n",
       " 1,\n",
       " 19490,\n",
       " 1,\n",
       " 8051,\n",
       " 1,\n",
       " 2208,\n",
       " 2941,\n",
       " 31952,\n",
       " 4,\n",
       " 1,\n",
       " 2625,\n",
       " 1,\n",
       " 2624,\n",
       " 20026,\n",
       " 31897,\n",
       " 2,\n",
       " 0,\n",
       " 13623,\n",
       " 1,\n",
       " 2924,\n",
       " 20027,\n",
       " 1,\n",
       " 3727,\n",
       " 2766,\n",
       " 20040,\n",
       " 1,\n",
       " 2635,\n",
       " 20026,\n",
       " 20368,\n",
       " 1,\n",
       " 2740,\n",
       " 20029,\n",
       " 1,\n",
       " 2817,\n",
       " 20017,\n",
       " 1,\n",
       " 4340,\n",
       " 20024,\n",
       " 1,\n",
       " 2303,\n",
       " 20026,\n",
       " 31828,\n",
       " 1,\n",
       " 2979,\n",
       " 20027,\n",
       " 1,\n",
       " 3420,\n",
       " 9417,\n",
       " 20040,\n",
       " 1,\n",
       " 2910,\n",
       " 10011,\n",
       " 20023,\n",
       " 1,\n",
       " 891,\n",
       " 1,\n",
       " 380,\n",
       " 20000,\n",
       " 20033,\n",
       " 1,\n",
       " 4877,\n",
       " 20036,\n",
       " 31974,\n",
       " 1,\n",
       " 3539,\n",
       " 1,\n",
       " 3058,\n",
       " 20018,\n",
       " 1,\n",
       " 2501,\n",
       " 1,\n",
       " 4415,\n",
       " 20030,\n",
       " 1,\n",
       " 2505,\n",
       " 1,\n",
       " 2685,\n",
       " 20026,\n",
       " 31897,\n",
       " 1,\n",
       " 0,\n",
       " 4925,\n",
       " 1,\n",
       " 2527,\n",
       " 20033,\n",
       " 3042,\n",
       " 20046,\n",
       " 1,\n",
       " 19481,\n",
       " 1,\n",
       " 4141,\n",
       " 20012,\n",
       " 1,\n",
       " 233,\n",
       " 20000,\n",
       " 1,\n",
       " 2473,\n",
       " 1,\n",
       " 3801,\n",
       " 20028,\n",
       " 20026,\n",
       " 31828,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2503,\n",
       " 1,\n",
       " 2508,\n",
       " 9238,\n",
       " 1,\n",
       " 3065,\n",
       " 4165,\n",
       " 1,\n",
       " 4085,\n",
       " 1,\n",
       " 2523,\n",
       " 20025,\n",
       " 1,\n",
       " 3377,\n",
       " 3852,\n",
       " 1,\n",
       " 3328,\n",
       " 3646,\n",
       " 12901,\n",
       " 20013,\n",
       " 31828,\n",
       " 1,\n",
       " 2464,\n",
       " 20031,\n",
       " 13079,\n",
       " 1,\n",
       " 2308,\n",
       " 20046,\n",
       " 7726,\n",
       " 1,\n",
       " 2716,\n",
       " 9814,\n",
       " 32267,\n",
       " 1,\n",
       " 2514,\n",
       " 20034,\n",
       " 1,\n",
       " 2196,\n",
       " 1,\n",
       " 2205,\n",
       " 20016,\n",
       " 1,\n",
       " 2581,\n",
       " 20013,\n",
       " 31952,\n",
       " 4,\n",
       " 1,\n",
       " 20024,\n",
       " 20026,\n",
       " 31897,\n",
       " 2,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Bugün Ali, sabah erken saatte İstanbul’un sakin bir köyünün kıyısındaki eski evinin kapısını yavaşça açıp bahçeye çıktı. Kitaptan not aldığı fikrini deftere yazdı ve “Bu projenin aşamaları uç uca eklenecek, sonra rapor olarak tek tek sunulacak,” dedi. Öğleye doğru kulüpte kısa bir toplantı yaptı; şefle ve arkadaşıyla yüz yüze görüşüp gelişecek işleri ayrıntılıca konuştu.\n",
    "Toplantıdan sonra taslağı evde tamamlayacak işleri listeleyecek ve yapacağı deneyleri planlayacaktı; ancak ağaçtan düşen küçücük bir dalcık ayağına değince bahçede kalıp ölçümleri bitirmeyi seçti. Yorgunluğu artınca, “Bu metnin ilk bölümünü kısacık tutayım, sonra uzunca açıklayayım,” diye düşündü.\n",
    "Akşamüstü arabacı komşusunun getirdiği parçalık malzemeyi atölyede denedi; taşçı ustanın önerisiyle küçücük delikleri büyütüp ince ayarlı bir ölçümcük daha ekledi. Nihayet geliyor gibi görünen sonuçlar onu sevindirdi; “Bu verilerin çoğunu yarın yeniden deneyip doğrulayacağım; yapmayıp beklersem geçecek zamanı boşa harcamış olurum,” dedi.\n",
    " \"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43455bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBugün Ali, sabah erken saatta İstanbul’ün sakin bir köyünın kıyısındaki eski evinın kapısına yavaşça açıp bahçeye çıktı. Kitaptan not aldığı fikrini deftere yazdı ve ▁u▁Bu projenin aşamaları uç uca eklenecek, sonra rapor olarak tek tek sunuylacak,▁u▁ dadı. Öğleye doğru kulüpte kısa bir toplantı yaptı; şefle ve arkadaşıyla yüz yüze görüşüp gelişecek işleri ayrıntılıca konuştu.\\nToplantıdan sonra taslağı evde tamamlanın işleri listeleyecek ve yapacağı deneyleri planlanındı; ancak ağaçtan düşen küçücük bir dalcık ayağına değince bahçede kalıp ölçümleri bitirmeyi seçti. Yorgunluğu artınca, ▁u▁Bu metnin ilk bölümünü kısacık tutayım, sonra uzunca açıklayayım,▁u▁ diye düşündü.\\nAkşamüstü arabacı komşusunun getirdiği parçalık malzemeyi atölyede denedi; taşçı ustanın önerisiyle küçücük delikleri büyütüp ince ayarlı bir ölçümcük daha ekledi. Nihayet geliyor gibi görünen sonuçlar ona sevindirdı; ▁u▁Bu verilerin çoğunu yarın yeniden deneyip doğrulayacağım; yapmayıp beklersem geçecek zamanı boşa harcamış olurum,▁u▁ dadı.\\n '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39f1af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBugün Ali, sabah erken saatta İstanbul’ün sakin bir köyünın kıyısındaki eski evinın kapısına yavaşça açıp bahçeye çıktı. Kitaptan not aldığı fikrini deftere yazdı ve ▁u▁Bu projenin aşamaları uc uca eklenecek, sonra rapor olarak tek tek sunuylacak,▁u▁ dadı. Öğleye doğru kulüpte kısa bir toplantı yapdı; şefle ve arkadaşıyla yüz yüze görüşüp gelişecek işleri ayrıntılıca konuşdı.\\nToplantıdan sonra taslağı evde tamamlanın işleri listeleyecek ve yapacağı deneyleri planlanındı; ancak ağaçtan düşen küçücük bir dalcığ ayağına değince bahçede kalıb ölçümleri bitirmeyi seçdı. Yorgunluğu artınca, ▁u▁Bu metnin ilk bölümünü kısacık tutayım, sonra uzunca açıklayayım,▁u▁ diye düşündı.\\nAkşamüstü arabacı komşusunun getirdıği parçalık malzemeyi atölyede denedı; taşçı ustanın önerisiyle küçücük delikleri büyütüp ince ayarlı bir ölçümcük daha ekledı. Nihayet geliyor gibi görünen sonuçlar ona sevindirdı; ▁u▁Bu verilerin çoğunu yarın yeniden deneyip doğrulayacağım; yapmayıp beklersem geçecek zamanı boşa harcamış olurum,▁u▁ dadı.\\n '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.gpt_decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d677259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2e457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"sıcak\" üçlü, yumuşama ve ünsüz düşmesi\n",
    "# \"bedir\" ünlü düşmesi\n",
    "# \"et\" yumuşama\n",
    "# \"açımla\" kalın genişleme\n",
    "# \"garipse\" ince genişleme\n",
    "# \"hade\" yanlış eklenmiş\n",
    "indexler = {\n",
    "  \"üçlü, yumuşama ve ünsüz düşmesi\": 0,\n",
    "  \"ünlü-düşme\": 0,\n",
    "  \"yumuşama\": 0,\n",
    "  \"kalın-genişleme\": 0,\n",
    "  \"ince-genişleme\": 0,\n",
    "  \"yanlış eklenmiş\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00033e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"v09_kokler.json\", \"r\") as f:\n",
    "    v09_kokler = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4f56d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'üçlü, yumuşama ve ünsüz düşmesi': 100,\n",
       "  'ünlü-düşme': 110,\n",
       "  'yumuşama': 198,\n",
       "  'kalın-genişleme': 2080,\n",
       "  'ince-genişleme': 2223,\n",
       "  'yanlış eklenmiş': 2315},\n",
       " 20000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v09_kokler = {}\n",
    "\n",
    "i = 0\n",
    "current_id = 0\n",
    "\n",
    "for key, value in v09_kokler.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_v09_kokler[key] = i\n",
    "  if key == \"sıcak\":\n",
    "    indexler[\"üçlü, yumuşama ve ünsüz düşmesi\"] = i\n",
    "  elif key == \"bedir\":\n",
    "    indexler[\"ünlü-düşme\"] = i\n",
    "  elif key == \"et\":\n",
    "    indexler[\"yumuşama\"] = i\n",
    "  elif key == \"açımla\":\n",
    "    indexler[\"kalın-genişleme\"] = i\n",
    "  elif key == \"garipse\":\n",
    "    indexler[\"ince-genişleme\"] = i\n",
    "  elif key == \"hade\":\n",
    "    indexler[\"yanlış eklenmiş\"] = i\n",
    "\n",
    "i += 1\n",
    "while i < 20000:\n",
    "  new_v09_kokler[f\"kok_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_kokler_new.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_v09_kokler, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "indexler, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "479b6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c78c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bpe_v05.json\", \"r\") as f:\n",
    "    bpe_tokens = json.load(f)\n",
    "\n",
    "with open(\"ekler_v05.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "with open(\"kokler_v07.json\", \"r\") as f:\n",
    "    roots = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c49b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20256\n"
     ]
    }
   ],
   "source": [
    "new_suffixes = {}\n",
    "i -= 1\n",
    "for key, value in suffixes.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_suffixes[key] = i\n",
    "\n",
    "i += 1\n",
    "\n",
    "while i < 20256:\n",
    "  new_suffixes[f\"ek_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_ekler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_suffixes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d35113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeteations = []\n",
    "\n",
    "for key in suffixes.keys():\n",
    "  if key in roots:\n",
    "    repeteations.append(key)\n",
    "    del roots[key]\n",
    "\n",
    "len(repeteations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4b2326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_roots = {}\n",
    "\n",
    "root_keys = list(roots.keys())\n",
    "for i in range(100):\n",
    "  new_roots[root_keys[i]] = i\n",
    "  del roots[root_keys[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bde7b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(roots_with_2_options):\n",
    "  for new_root in roots_with_2_options[i]:\n",
    "    new_roots[new_root] = i + 100\n",
    "    del roots[new_root]\n",
    "  i += 1\n",
    "i = i + 100\n",
    "k = i\n",
    "\n",
    "j = 0\n",
    "while i < k + len(roots_with_3_options):\n",
    "  for new_root in roots_with_3_options[j]:\n",
    "    new_roots[new_root] = i\n",
    "    del roots[new_root]\n",
    "  i += 1\n",
    "  j += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "161ccd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19529 çüş 20032\n",
      "19529\n",
      "20000\n",
      "20256\n"
     ]
    }
   ],
   "source": [
    "current_id = 0\n",
    "i = 2502\n",
    "for key, value in roots.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_roots[key] = i\n",
    "print(i, key, value)\n",
    "print(i)\n",
    "while i < 20000:\n",
    "  i = i + 1\n",
    "  new_roots[f\"kok_temp_{i}\"] = i\n",
    "\n",
    "print(i)\n",
    "\n",
    "with open(\"v09_kokler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_roots, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "new_suffixes = {}\n",
    "for key, value in suffixes.items():\n",
    "  if value != current_id:\n",
    "    current_id = value\n",
    "    i += 1\n",
    "  new_suffixes[key] = i\n",
    "\n",
    "i += 1\n",
    "\n",
    "while i < 20256:\n",
    "  new_suffixes[f\"ek_temp_{i}\"] = i\n",
    "  i += 1\n",
    "\n",
    "with open(\"v09_ekler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(new_suffixes, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3715bc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938fbef4434a43708ed2bfe3434b8e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00004-of-00010.parquet:   0%|          | 0.00/224M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e527879372948a9acdaa7cdc0a56be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00005-of-00010.parquet:   0%|          | 0.00/237M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25fd4e7649347f0ae4e39afb4bd0bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00006-of-00010.parquet:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720d6da4f0c419eb8d8800de996e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00007-of-00010.parquet:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5e779748134c77ae9bf9a6ce4c5c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00008-of-00010.parquet:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd12c1f24b34dba83151ee4c342a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20250703.tr/train-00009-of-00010.parquet:   0%|          | 0.00/149M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ce957054c43f98a13f63a0ff1f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'raw_mediawiki', 'text'],\n",
       "    num_rows: 641443\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English dataset from the latest dump\n",
    "ds_wiki = load_dataset(\"omarkamali/wikipedia-monthly\", \"20250703.tr\", split=\"train\")\n",
    "ds_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28b4511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0c57efdafe40acb0d2552747047e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6445abbec73e41bfad0c35c3b177878c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91818713cc14e21ade101810661b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2458c413264da78d70fe39f84fc791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2563449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Puan', 'Tarih', 'Baslik', 'Yorum', 'Konum', 'Yoruma verilen artı sayısı', 'Yoruma verilen eksi sayısı'],\n",
       "        num_rows: 2563449\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_yorumbudur = load_dataset(\"alibayram/yorumbudur\")\n",
    "ds_yorumbudur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24dc1229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182fe0c34cb4095835d501b8c818369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09f7437662d4cc091a6ba1f1d2ec86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hepsiburadaYorumlar.parquet:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87095dcdb57a4590a98b28de4b975d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2657073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Puan', 'Baslik', 'Yorum'],\n",
       "        num_rows: 2657073\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_hepsiburada = load_dataset(\"alibayram/hepsiburada_yorumlar\")\n",
    "ds_hepsiburada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73712f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_wiki 0\n",
      "ds_wiki 50000\n",
      "ds_wiki 100000\n",
      "ds_wiki 150000\n",
      "ds_wiki 200000\n",
      "ds_wiki 250000\n",
      "ds_wiki 300000\n",
      "ds_wiki 350000\n",
      "ds_wiki 400000\n",
      "ds_wiki 450000\n",
      "ds_wiki 500000\n",
      "ds_wiki 550000\n",
      "ds_wiki 600000\n",
      "641443\n",
      "ds_yorumbudur 0\n",
      "ds_yorumbudur 50000\n",
      "ds_yorumbudur 100000\n",
      "ds_yorumbudur 150000\n",
      "ds_yorumbudur 200000\n",
      "ds_yorumbudur 250000\n",
      "ds_yorumbudur 300000\n",
      "ds_yorumbudur 350000\n",
      "ds_yorumbudur 400000\n",
      "ds_yorumbudur 450000\n",
      "ds_yorumbudur 500000\n",
      "ds_yorumbudur 550000\n",
      "ds_yorumbudur 600000\n",
      "ds_yorumbudur 650000\n",
      "ds_yorumbudur 700000\n",
      "ds_yorumbudur 750000\n",
      "ds_yorumbudur 800000\n",
      "ds_yorumbudur 850000\n",
      "ds_yorumbudur 900000\n",
      "ds_yorumbudur 950000\n",
      "ds_yorumbudur 1000000\n",
      "ds_yorumbudur 1050000\n",
      "ds_yorumbudur 1100000\n",
      "ds_yorumbudur 1150000\n",
      "ds_yorumbudur 1200000\n",
      "ds_yorumbudur 1250000\n",
      "ds_yorumbudur 1300000\n",
      "ds_yorumbudur 1350000\n",
      "ds_yorumbudur 1400000\n",
      "ds_yorumbudur 1450000\n",
      "ds_yorumbudur 1500000\n",
      "ds_yorumbudur 1550000\n",
      "ds_yorumbudur 1600000\n",
      "ds_yorumbudur 1650000\n",
      "ds_yorumbudur 1700000\n",
      "ds_yorumbudur 1750000\n",
      "ds_yorumbudur 1800000\n",
      "ds_yorumbudur 1850000\n",
      "ds_yorumbudur 1900000\n",
      "ds_yorumbudur 1950000\n",
      "ds_yorumbudur 2000000\n",
      "ds_yorumbudur 2050000\n",
      "ds_yorumbudur 2100000\n",
      "ds_yorumbudur 2150000\n",
      "ds_yorumbudur 2200000\n",
      "ds_yorumbudur 2250000\n",
      "ds_yorumbudur 2300000\n",
      "ds_yorumbudur 2350000\n",
      "ds_yorumbudur 2400000\n",
      "ds_yorumbudur 2450000\n",
      "ds_yorumbudur 2500000\n",
      "ds_yorumbudur 2550000\n",
      "3204892\n",
      "ds_hepsiburada 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_hepsiburada[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m   text = \u001b[43mds_hepsiburada\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYorum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     16\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (i % \u001b[32m50000\u001b[39m == \u001b[32m0\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mds_hepsiburada\u001b[39m\u001b[33m\"\u001b[39m, i)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for i in range(len(ds_wiki)):\n",
    "  text = ds_wiki[i]['text'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_wiki\", i)\n",
    "  texts.append(text)\n",
    "print(len(texts))\n",
    "for i in range(len(ds_yorumbudur['train'])):\n",
    "  text = ds_yorumbudur['train'][i]['Yorum'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_yorumbudur\", i)\n",
    "  texts.append(text)\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c75c9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_hepsiburada 0\n",
      "ds_hepsiburada 50000\n",
      "ds_hepsiburada 100000\n",
      "ds_hepsiburada 150000\n",
      "ds_hepsiburada 200000\n",
      "ds_hepsiburada 250000\n",
      "ds_hepsiburada 300000\n",
      "ds_hepsiburada 350000\n",
      "ds_hepsiburada 400000\n",
      "ds_hepsiburada 450000\n",
      "ds_hepsiburada 500000\n",
      "ds_hepsiburada 550000\n",
      "ds_hepsiburada 600000\n",
      "ds_hepsiburada 650000\n",
      "ds_hepsiburada 700000\n",
      "ds_hepsiburada 750000\n",
      "ds_hepsiburada 800000\n",
      "ds_hepsiburada 850000\n",
      "ds_hepsiburada 900000\n",
      "ds_hepsiburada 950000\n",
      "ds_hepsiburada 1000000\n",
      "ds_hepsiburada 1050000\n",
      "ds_hepsiburada 1100000\n",
      "ds_hepsiburada 1150000\n",
      "ds_hepsiburada 1200000\n",
      "ds_hepsiburada 1250000\n",
      "ds_hepsiburada 1300000\n",
      "ds_hepsiburada 1350000\n",
      "ds_hepsiburada 1400000\n",
      "ds_hepsiburada 1450000\n",
      "ds_hepsiburada 1500000\n",
      "ds_hepsiburada 1550000\n",
      "ds_hepsiburada 1600000\n",
      "ds_hepsiburada 1650000\n",
      "ds_hepsiburada 1700000\n",
      "ds_hepsiburada 1750000\n",
      "ds_hepsiburada 1800000\n",
      "ds_hepsiburada 1850000\n",
      "ds_hepsiburada 1900000\n",
      "ds_hepsiburada 1950000\n",
      "ds_hepsiburada 2000000\n",
      "ds_hepsiburada 2050000\n",
      "ds_hepsiburada 2100000\n",
      "ds_hepsiburada 2150000\n",
      "ds_hepsiburada 2200000\n",
      "ds_hepsiburada 2250000\n",
      "ds_hepsiburada 2300000\n",
      "ds_hepsiburada 2350000\n",
      "ds_hepsiburada 2400000\n",
      "ds_hepsiburada 2450000\n",
      "ds_hepsiburada 2500000\n",
      "ds_hepsiburada 2550000\n",
      "ds_hepsiburada 2600000\n",
      "ds_hepsiburada 2650000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5879363"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(ds_hepsiburada['train'])):\n",
    "  if ds_hepsiburada['train'][i]['Yorum'] == None:\n",
    "    continue\n",
    "  text = ds_hepsiburada['train'][i]['Yorum'].lower()\n",
    "  if (i % 50000 == 0):\n",
    "    print(\"ds_hepsiburada\", i)\n",
    "  texts.append(text)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f1d674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"film şu anlamlara gelebilir:\\n\\n camlara yapıştırılarak içerinin görünmesini engelleyen bir tür ince yaprak\\n sinemacılıkta, bir oyunun bütününü taşıyan şerit veya şeritlerin bütünü\\n film (fotoğrafçılık), fotoğrafçılıkta, radyografide ve sinemacılıkta resim çekmek için kullanılan; selülozdan, saydam, bükülebilir şerit\\n film (sinema), sinema makinesiyle gösterilen eser, izleti\\n film (film), samuel beckett'in yazdığı tek senaryodan çekilen 1965 abd yapımı film\\n total film, 1997'den beri i̇ngiltere'de yayımlanan sinema dergisi\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2387f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.add_tokens(list(new_roots.keys()))\n",
    "tokenizer.add_tokens(list(new_suffixes.keys()))\n",
    "trainer = BpeTrainer(vocab_size=40000)\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "\n",
    "with open(\"v09_bpe_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(tokenizer.get_vocab(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "368df881",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tv09_raw_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efee8d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokens = {}\n",
    "i = 20256\n",
    "\n",
    "tokenizer_vocab = tokenizer.get_vocab().keys()\n",
    "\n",
    "# filter keys that shorter than 2 characters\n",
    "tokenizer_vocab = [key for key in tokenizer_vocab if len(key) > 1]\n",
    "\n",
    "# sort tokenizer_vocab by length of the key it is string short to long\n",
    "tokenizer_vocab = sorted(tokenizer_vocab, key=len)\n",
    "\n",
    "for key in tokenizer_vocab:\n",
    "  if key in new_suffixes.keys():\n",
    "    continue\n",
    "  if key in new_roots.keys():\n",
    "    continue\n",
    "  bpe_tokens[key] = i\n",
    "  i += 1\n",
    "  if i >= 31768:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9116a141f8454dbcc4b378c31168f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e7fff5ef68418585ebac8548319213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a39be6985f493e837403aaab0ace38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/267M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9d226daf3c49f8945a38cf7874f935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/317423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "317423"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds_haber = load_dataset(\"habanoz/24-news-tr-v1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(len(ds_haber['train'])):\n",
    "  text = ds_haber['train'][i]['Text'].lower()\n",
    "  texts.append(text)\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e77aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m new_roots.keys():\n\u001b[32m     21\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer_vocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m():\n\u001b[32m     23\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     24\u001b[39m bpe_tokens[key] = i\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "news_tokenizer = Tokenizer(BPE())\n",
    "news_tokenizer.pre_tokenizer = Whitespace()\n",
    "news_tokenizer.add_tokens(list(new_roots.keys()))\n",
    "news_tokenizer.add_tokens(list(new_suffixes.keys()))\n",
    "news_trainer = BpeTrainer(vocab_size=40000)\n",
    "\n",
    "\n",
    "news_tokenizer.train_from_iterator(texts, trainer=news_trainer)\n",
    "\n",
    "bpe_tokens = {}\n",
    "\n",
    "news_tokenizer_vocab = news_tokenizer.get_vocab().keys()\n",
    "\n",
    "# sort tokenizer_vocab by length of the key it is string short to long\n",
    "news_tokenizer_vocab = sorted(news_tokenizer_vocab, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf81b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in news_tokenizer_vocab:\n",
    "  if key in new_suffixes.keys():\n",
    "    continue\n",
    "  if key in new_roots.keys():\n",
    "    continue\n",
    "  if key in tokenizer_vocab:\n",
    "    continue\n",
    "  bpe_tokens[key] = i\n",
    "  i += 1\n",
    "  if i >= 32768:\n",
    "    break\n",
    "\n",
    "\n",
    "with open(\"v09_bpe_tokens.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(bpe_tokens, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c81bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['a', 'a']\n",
      "['a', 'a', 'd']\n",
      "['a', 'a', 'd', 'sf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([22274, 22274, 32266, 20568], ['a', 'a', 'd', 'sf'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"aadsfasdfAliAtaBak\"\n",
    "uppercase_indices = [i for i, c in enumerate(word) if c.isupper()]\n",
    "word = word.lower()\n",
    "\n",
    "# check if word is a root if not, recursively drop the last letter until it is a root\n",
    "# or it is 2 letters then check if it is a suffix recursively by dropping the last letter\n",
    "# if the remaining part is not root or suffix then check if it is a bpe token\n",
    "# and remove that part from the word and continue with the remaining part\n",
    "\n",
    "tokens = []\n",
    "ids = []\n",
    "\n",
    "def get_longest_root_id(word: str) -> tuple[int, str]:\n",
    "  if word in roots:\n",
    "    return roots[word], word\n",
    "  if len(word) <= 2:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_longest_root_id(word[:-1])\n",
    "\n",
    "id, root = get_longest_root_id(word)\n",
    "\n",
    "while len(word) > 2 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(root)\n",
    "  word = word[len(root):]\n",
    "  print(tokens)\n",
    "  id, root = get_longest_root_id(word)\n",
    "\n",
    "def get_longest_suffix_id(word: str) -> tuple[int, str]:\n",
    "  if word in suffixes:\n",
    "    return suffixes[word], word\n",
    "  if len(word) <= 1:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_longest_suffix_id(word[:-1])\n",
    "\n",
    "id, suffix = get_longest_suffix_id(word)\n",
    "\n",
    "while len(word) > 1 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(suffix)\n",
    "  word = word[len(suffix):]\n",
    "  print(tokens)\n",
    "  id, suffix = get_longest_suffix_id(word)\n",
    "\n",
    "def get_bpe_id(word: str) -> tuple[int, str]:\n",
    "  if word in bpe_tokens:\n",
    "    return bpe_tokens[word], word\n",
    "  if len(word) <= 1:\n",
    "    # not found\n",
    "    return None, word\n",
    "  return get_bpe_id(word[:-1])\n",
    "\n",
    "id, bpe = get_bpe_id(word)\n",
    "\n",
    "while len(word) > 0 and id is not None:\n",
    "  ids.append(id)\n",
    "  tokens.append(bpe)\n",
    "  word = word[len(bpe):]\n",
    "  print(tokens)\n",
    "  id, bpe = get_bpe_id(word)\n",
    "\n",
    "ids, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "339cf924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _uppercase_indices(text: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start in the text.\"\"\"\n",
    "    return [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "def tokenize_word(s: str) -> List[dict]:\n",
    "    \"\"\"Tokenize a single word without camel splitting.\"\"\"\n",
    "    result: List[dict] = []\n",
    "\n",
    "    while s:\n",
    "        # 1) Try ROOT\n",
    "        rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "        if rid is not None:\n",
    "            result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "            s = s[len(rtok):]\n",
    "            continue\n",
    "\n",
    "        # 2) Try SUFFIX\n",
    "        sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "        if sid is not None:\n",
    "            result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "            s = s[len(stok):]\n",
    "            continue\n",
    "\n",
    "        # 3) Try BPE\n",
    "        bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "        if bid is not None:\n",
    "            result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "            s = s[len(btok):]\n",
    "            continue\n",
    "\n",
    "        # 4) No match → UNK for one char\n",
    "        result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "        s = s[1:]\n",
    "\n",
    "    return result\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize whole text, splitting from spaces only.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    upper_idx = _uppercase_indices(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for idx, word in enumerate(words):\n",
    "        if word != \"\":\n",
    "            final_tokens.extend(tokenize_word(word))\n",
    "        if idx < len(words) - 1:\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, upper_idx\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7b38600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}]\n",
      "[11, 14, 17]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "        # after each segment put a space token if it is not the last segment\n",
    "        result.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eaa86efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'as', 'id': 1249, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': 'li', 'id': 22324, 'type': 'SUFFIX'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<', 'id': 32086, 'type': 'BPE'}, {'token': 'start', 'id': 3208, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'of', 'id': 20030, 'type': 'ROOT'}, {'token': '_', 'id': 31905, 'type': 'BPE'}, {'token': 'ima', 'id': 1600, 'type': 'ROOT'}, {'token': 'ge', 'id': 22332, 'type': 'SUFFIX'}, {'token': '>', 'id': 32112, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}]\n",
      "[11, 12, 14, 21, 24]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "        # after each segment put a space token if it is not the last segment\n",
    "        result.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAAaAasdfliAtaBak<start_of_image> başka bir şey\")\n",
    "print(tokens)\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "410af126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 'a', 'id': 22274, 'type': 'SUFFIX'}, {'token': 'ad', 'id': 1053, 'type': 'ROOT'}, {'token': '<unknown>', 'id': 4, 'type': 'ROOT'}, {'token': 's', 'id': 22323, 'type': 'SUFFIX'}, {'token': 'fas', 'id': 5488, 'type': 'ROOT'}, {'token': 'df', 'id': 20573, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'ali', 'id': 327, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'ata', 'id': 471, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'bak', 'id': 445, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'h', 'id': 32012, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 't', 'id': 32053, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 't', 'id': 32053, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'p', 'id': 31974, 'type': 'BPE'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'server', 'id': 3883, 'type': 'ROOT'}, {'token': '.', 'id': 31897, 'type': 'BPE'}, {'token': 'ca', 'id': 1212, 'type': 'ROOT'}, {'token': 'll', 'id': 20489, 'type': 'BPE'}, {'token': '()', 'id': 20803, 'type': 'BPE'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': 'başka', 'id': 237, 'type': 'ROOT'}, {'token': '<space>', 'id': 1, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'bir', 'id': 100, 'type': 'ROOT'}, {'token': '<uppercase>', 'id': 0, 'type': 'ROOT'}, {'token': 'şey', 'id': 156, 'type': 'ROOT'}]\n",
      "[11, 14, 17, 20, 21, 22, 23, 24, 44, 47]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class TokenType:\n",
    "    ROOT = \"ROOT\"\n",
    "    SUFFIX = \"SUFFIX\"\n",
    "    BPE = \"BPE\"\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Return indices where uppercase letters start.\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(word: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Returns:\n",
    "        tokens: list of dicts {'token': str, 'id': int, 'type': enum}\n",
    "        uppercase_indices: list of int positions of uppercase letters in the original word\n",
    "    \"\"\"\n",
    "    uppercase_indices = _camel_split_indices(word)\n",
    "    result: List[dict] = []\n",
    "\n",
    "    # Track current position in the ORIGINAL word so we know\n",
    "    # whether a segment starts with an uppercase letter.\n",
    "    orig_pos = 0\n",
    "\n",
    "    for seg in _split_camel(word):  # seg is lowercased\n",
    "        # If the current segment starts at an uppercase in the original word,\n",
    "        # emit the <uppercase> marker (id=0) BEFORE tokenizing the segment.\n",
    "        if orig_pos < len(word) and word[orig_pos].isupper():\n",
    "            result.append({\"token\": \"<uppercase>\", \"id\": 0, \"type\": TokenType.ROOT})\n",
    "\n",
    "        s = seg\n",
    "        while s:\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                result.append({\"token\": rtok, \"id\": rid, \"type\": TokenType.ROOT})\n",
    "                s = s[len(rtok):]\n",
    "                orig_pos += len(rtok)\n",
    "                continue\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                result.append({\"token\": stok, \"id\": sid, \"type\": TokenType.SUFFIX})\n",
    "                s = s[len(stok):]\n",
    "                orig_pos += len(stok)\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                result.append({\"token\": btok, \"id\": bid, \"type\": TokenType.BPE})\n",
    "                s = s[len(btok):]\n",
    "                orig_pos += len(btok)\n",
    "                continue\n",
    "\n",
    "            # 4) No match → UNK for one char (advance orig_pos by 1)\n",
    "            result.append({\"token\": \"<unknown>\", \"id\": 4, \"type\": TokenType.ROOT})\n",
    "            s = s[1:]\n",
    "            orig_pos += 1\n",
    "\n",
    "    return result, uppercase_indices\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> Tuple[List[dict], List[int]]:\n",
    "    \"\"\"Tokenize full text, preserving spaces.\"\"\"\n",
    "    final_tokens: List[dict] = []\n",
    "    uppercase_indices: List[int] = [i for i, c in enumerate(text) if c.isupper()]\n",
    "\n",
    "    parts = text.split(\" \")\n",
    "    for idx, part in enumerate(parts):\n",
    "        if part.strip():  # non-empty\n",
    "            tokens, _ = tokenize_word(part)\n",
    "            final_tokens.extend(tokens)\n",
    "        if idx < len(parts) - 1:  # space between words\n",
    "            final_tokens.append({\"token\": \"<space>\", \"id\": 1, \"type\": TokenType.ROOT})\n",
    "\n",
    "    return final_tokens, uppercase_indices\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# roots = {\"ali\": 1, \"ata\": 2, \"bak\": 3}\n",
    "# suffixes = {\"lar\": 4, \"da\": 5}\n",
    "# bpe_tokens = {\"aa\": 6, \"ds\": 7, \"fa\": 8}\n",
    "tokens, upper_idx = tokenize_text(\"▁aad▁sfasdfAliAtaBakHTTPServer.call() başka BirŞey\")\n",
    "print(tokens)       # includes \"<uppercase>\" before \"ali\", \"bir\", and \"şey\" segments that start uppercase\n",
    "print(upper_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0d1deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(tokens, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4277a8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1249, 20573, 1053, 32078, 22268, 32222, 31768, 32072, 32037, 32037, 31832, 31832, 32149, 32037, 22323] ['as', 'df', 'ad', '$', '123', '~', 'q', 'w', 'æ', 'æ', '@', '@', '®', 'æ', 's']\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Example dicts (fill with your real mappings)\n",
    "# roots = {\"ali\": 101, \"ata\": 102, \"bak\": 103, ...}\n",
    "# suffixes = {\"lar\": 201, \"ler\": 202, \"da\": 203, \"de\": 204, ...}\n",
    "# bpe_tokens = {\"aa\": 301, \"ds\": 302, \"fa\": 303, ...}\n",
    "\n",
    "def _longest_prefix_lookup(s: str, table: Dict[str, int]) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Return (id, token_str) of the longest prefix of s found in table; else (None, '').\"\"\"\n",
    "    # Check longest to shortest prefix\n",
    "    for end in range(len(s), 0, -1):\n",
    "        cand = s[:end]\n",
    "        if cand in table:\n",
    "            return table[cand], cand\n",
    "    return None, \"\"\n",
    "\n",
    "def _camel_split_indices(word: str) -> List[int]:\n",
    "    \"\"\"Indices where uppercase letters start (CamelCase boundaries).\"\"\"\n",
    "    return [i for i, c in enumerate(word) if c.isupper()]\n",
    "\n",
    "def _split_camel(word: str) -> List[str]:\n",
    "    \"\"\"Split word on uppercase indices, keep original order, lowercase parts.\"\"\"\n",
    "    idxs = _camel_split_indices(word)\n",
    "    if not idxs:\n",
    "        return [word.lower()]\n",
    "    parts = []\n",
    "    last = 0\n",
    "    for i in idxs:\n",
    "        if i > last:\n",
    "            parts.append(word[last:i].lower())\n",
    "        last = i\n",
    "    parts.append(word[last:].lower())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def tokenize_word(\n",
    "    word: str,\n",
    "    roots: Dict[str, int],\n",
    "    suffixes: Dict[str, int],\n",
    "    bpe_tokens: Dict[str, int],\n",
    "    unk_token: str = \"<unknown>\"\n",
    ") -> Tuple[List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a word with priority:\n",
    "      1) roots (longest prefix)\n",
    "      2) suffixes (longest prefix)\n",
    "      3) BPE (longest prefix)\n",
    "    After consuming suffix -> re-check roots.\n",
    "    After consuming BPE   -> re-check roots then suffixes.\n",
    "    Repeats until segment is exhausted; falls back to <UNK> for 1 char if no match.\n",
    "    Handles CamelCase by splitting and tokenizing each segment independently.\n",
    "    \"\"\"\n",
    "    all_ids: List[int] = []\n",
    "    all_tokens: List[str] = []\n",
    "\n",
    "    for seg in _split_camel(word):\n",
    "        s = seg\n",
    "        while s:\n",
    "            progressed = False\n",
    "\n",
    "            # 1) Try ROOT\n",
    "            rid, rtok = _longest_prefix_lookup(s, roots)\n",
    "            if rid is not None:\n",
    "                all_ids.append(rid)\n",
    "                all_tokens.append(rtok)\n",
    "                s = s[len(rtok):]\n",
    "                progressed = True\n",
    "                continue  # after root, we try root again on the new remainder\n",
    "\n",
    "            # 2) Try SUFFIX\n",
    "            sid, stok = _longest_prefix_lookup(s, suffixes)\n",
    "            if sid is not None:\n",
    "                all_ids.append(sid)\n",
    "                all_tokens.append(stok)\n",
    "                s = s[len(stok):]\n",
    "                progressed = True\n",
    "                # IMPORTANT: after suffix, loop restarts and tries ROOT again first\n",
    "                continue\n",
    "\n",
    "            # 3) Try BPE\n",
    "            bid, btok = _longest_prefix_lookup(s, bpe_tokens)\n",
    "            if bid is not None:\n",
    "                all_ids.append(bid)\n",
    "                all_tokens.append(btok)\n",
    "                s = s[len(btok):]\n",
    "                progressed = True\n",
    "                # IMPORTANT: after BPE, loop restarts and tries ROOT → SUFFIX again\n",
    "                continue\n",
    "\n",
    "            # 4) Nothing matched: emit UNK for one character to avoid stalling\n",
    "            all_tokens.append(unk_token)\n",
    "            all_ids.append(-1)  # or your UNK id\n",
    "            s = s[1:]\n",
    "            progressed = True\n",
    "\n",
    "            if not progressed:\n",
    "                # safety (shouldn't happen due to UNK logic)\n",
    "                break\n",
    "\n",
    "    return all_ids, all_tokens\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "text_with_unkown_token = \"asdfad$123~qwææ@@®æs\"\n",
    "ids, toks = tokenize_word(text_with_unkown_token, roots, suffixes, bpe_tokens)\n",
    "print(ids, toks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrTokenizer:\n",
    "  def __init__(self,\n",
    "               roots: dict[str, int],\n",
    "               suffixes: dict[str, int],\n",
    "               bpe_tokens: dict[str, int]\n",
    "              ):\n",
    "    self.roots = roots\n",
    "    self.suffixes = suffixes\n",
    "    self.bpe_tokens = bpe_tokens\n",
    "\n",
    "  def _get_root_id(self, word: str) -> tuple[int, str]:\n",
    "    if word in self.roots:\n",
    "      return self.roots[word], word\n",
    "    if len(word) == 2:\n",
    "      # not found\n",
    "      return None, word\n",
    "    return self._get_root_id(word[:-1])\n",
    "\n",
    "  def tokenize(self, text: str) -> list[int]:\n",
    "    tokens = []\n",
    "    uppercase_indices = [i for i, c in enumerate(text) if c.isupper()]\n",
    "    text = text.lower()\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for word in words:\n",
    "      # check if word is a root if not, recursively drop the last letter until it is a root\n",
    "      # or it is 2 letters then check if it is a suffix recursively by dropping the last letter\n",
    "      # if the remaining part is not root or suffix then check if it is a bpe token\n",
    "      # and remove that part from the word and continue with the remaining part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bed4e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['er',\n",
       " 'hab',\n",
       " 'a',\n",
       " ',',\n",
       " 'ben',\n",
       " 'li',\n",
       " 'ayr',\n",
       " 'a',\n",
       " 'm',\n",
       " '.',\n",
       " 'en',\n",
       " 'bir',\n",
       " 'yazılım',\n",
       " 'geliştir',\n",
       " 'i',\n",
       " 'ci',\n",
       " 'yi',\n",
       " 'm',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Merhaba, ben Ali Bayram. Ben bir yazılım geliştiriciyim.\"\n",
    "\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "\n",
    "tokens.tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
