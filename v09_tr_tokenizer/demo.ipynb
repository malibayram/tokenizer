{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21cc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/       demo.ipynb         kokler.json        tr_tokenizer.py\n",
      "bpe_tokenler.json  ekler.json         tr_decoder.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c31e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 0,\n",
       " 2697,\n",
       " 2,\n",
       " 0,\n",
       " 2212,\n",
       " 2,\n",
       " 0,\n",
       " 2794,\n",
       " 1,\n",
       " 2,\n",
       " 18194,\n",
       " 20043,\n",
       " 2,\n",
       " 6766,\n",
       " 20000,\n",
       " 2,\n",
       " 17321,\n",
       " 20000,\n",
       " 2,\n",
       " 20024,\n",
       " 2,\n",
       " 2595,\n",
       " 20024,\n",
       " 2,\n",
       " 2627,\n",
       " 2,\n",
       " 20024,\n",
       " 2,\n",
       " 3045,\n",
       " 20024,\n",
       " 2,\n",
       " 227,\n",
       " 20024,\n",
       " 2,\n",
       " 227,\n",
       " 15247,\n",
       " 2,\n",
       " 2656,\n",
       " 10572,\n",
       " 2,\n",
       " 2503,\n",
       " 2,\n",
       " 2599,\n",
       " 20038,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 165,\n",
       " 20021,\n",
       " 20035,\n",
       " 2,\n",
       " 20064,\n",
       " 4373,\n",
       " 20002,\n",
       " 3,\n",
       " 0,\n",
       " 165,\n",
       " 2,\n",
       " 165,\n",
       " 20037,\n",
       " 2,\n",
       " 2501,\n",
       " 2,\n",
       " 3303,\n",
       " 4,\n",
       " 2502,\n",
       " 20026,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tr_tokenizer import TRTokenizer\n",
    "\n",
    "tokenizer = TRTokenizer()\n",
    "\n",
    "text = \"\"\"\n",
    "Ali Ata Bak▁ aliler ahmetler selmanlar da bizde onlar da testte kitapta kitabını okudum bu işe\n",
    " \n",
    " burnunu sokma\n",
    "Burun buruna bir kaza\\toldu\n",
    " \"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a89f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29352c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cartroundponzaşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşirazşiraz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gemma_model import GemmaForCausalLM, get_config_for_270m_tr_tokenizer\n",
    "\n",
    "config_270m = get_config_for_270m_tr_tokenizer(\"float32\")\n",
    "\n",
    "gemma_model = GemmaForCausalLM(config_270m, tokenizer)\n",
    "# gemma_model.load_weights_from_hf(model.model.state_dict())\n",
    "gemma_model.from_pretrained(\"gemma-3-270m-tr-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85a5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sismografsismografsismografsismografsismografoglemuamıslanmaıslanmasümüksütemlikkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkullnkulln'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.generate(\"Merhaba, nasıl yardımcı olabilirim?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a3e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "gemma_model.embedder = nn.Embedding(32768, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7c02ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (embedder): Embedding(32768, 640)\n",
       "  (model): GemmaModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (query_norm): RMSNorm()\n",
       "          (key_norm): RMSNorm()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (pre_feedforward_layernorm): RMSNorm()\n",
       "        (post_feedforward_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60b9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_model.save_pretrained(\"gemma-3-270m-tr-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7490419b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,  79636, 236764,  56921, 118097,  85310,    548, 236881,    107,\n",
       "         104469, 164694, 236764,  39152,  40611,   3482,  41753, 169890,   7908,\n",
       "         236761,    108,    206,   2060,    520,    107,    215,    108, 236776,\n",
       "           1831]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensors = tokenizer.encode(\"Merhaba, nasıl yardımcı olabilirim?\")\n",
    "tensors = torch.tensor(tensors)\n",
    "tensors = tensors.unsqueeze(0)\n",
    "tensors = tensors.to(\"cpu\")\n",
    "\n",
    "ids = model.generate(tensors)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54019b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Merhaba, nasıl yardımcı olabilirim?\\nSoruyorum, ilk önce bir şekilde yazdım.\\n\\n<code>return m\\n</code>\\n\\nAral'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509528d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-pt\")\n",
    "model_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac7a399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb86fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained(\"gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958463f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_tokens.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_norm.weight', 'layers.0.self_attn.k_norm.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.input_layernorm.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.pre_feedforward_layernorm.weight', 'layers.0.post_feedforward_layernorm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_norm.weight', 'layers.1.self_attn.k_norm.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.pre_feedforward_layernorm.weight', 'layers.1.post_feedforward_layernorm.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_norm.weight', 'layers.2.self_attn.k_norm.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.pre_feedforward_layernorm.weight', 'layers.2.post_feedforward_layernorm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_norm.weight', 'layers.3.self_attn.k_norm.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.pre_feedforward_layernorm.weight', 'layers.3.post_feedforward_layernorm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_norm.weight', 'layers.4.self_attn.k_norm.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.pre_feedforward_layernorm.weight', 'layers.4.post_feedforward_layernorm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_norm.weight', 'layers.5.self_attn.k_norm.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.pre_feedforward_layernorm.weight', 'layers.5.post_feedforward_layernorm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_norm.weight', 'layers.6.self_attn.k_norm.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.pre_feedforward_layernorm.weight', 'layers.6.post_feedforward_layernorm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_norm.weight', 'layers.7.self_attn.k_norm.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.pre_feedforward_layernorm.weight', 'layers.7.post_feedforward_layernorm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_norm.weight', 'layers.8.self_attn.k_norm.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.pre_feedforward_layernorm.weight', 'layers.8.post_feedforward_layernorm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_norm.weight', 'layers.9.self_attn.k_norm.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.pre_feedforward_layernorm.weight', 'layers.9.post_feedforward_layernorm.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_norm.weight', 'layers.10.self_attn.k_norm.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.pre_feedforward_layernorm.weight', 'layers.10.post_feedforward_layernorm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_norm.weight', 'layers.11.self_attn.k_norm.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.pre_feedforward_layernorm.weight', 'layers.11.post_feedforward_layernorm.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_norm.weight', 'layers.12.self_attn.k_norm.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.pre_feedforward_layernorm.weight', 'layers.12.post_feedforward_layernorm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_norm.weight', 'layers.13.self_attn.k_norm.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.pre_feedforward_layernorm.weight', 'layers.13.post_feedforward_layernorm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_norm.weight', 'layers.14.self_attn.k_norm.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.pre_feedforward_layernorm.weight', 'layers.14.post_feedforward_layernorm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_norm.weight', 'layers.15.self_attn.k_norm.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.pre_feedforward_layernorm.weight', 'layers.15.post_feedforward_layernorm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_norm.weight', 'layers.16.self_attn.k_norm.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.pre_feedforward_layernorm.weight', 'layers.16.post_feedforward_layernorm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_norm.weight', 'layers.17.self_attn.k_norm.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.pre_feedforward_layernorm.weight', 'layers.17.post_feedforward_layernorm.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.state_dict().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645831b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0073,  0.0430, -0.0342, -0.0210, -0.0120,  0.0289,  0.0051, -0.0322,\n",
       "         0.0381, -0.0128], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "833c65a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0073,  0.0430, -0.0342, -0.0210, -0.0120,  0.0289,  0.0051, -0.0322,\n",
       "         0.0381, -0.0128], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512556d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['local_freqs_cis', 'global_freqs_cis']...\n",
      "Successfully loaded 236 weights\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67a9824",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgemma_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMerhaba, nasıl yardımcı olabilirim?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/v09_tr_tokenizer/gemma_model.py:578\u001b[39m, in \u001b[36mGemmaForCausalLM.generate\u001b[39m\u001b[34m(self, prompts, device, output_len, temperature, top_p, top_k)\u001b[39m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# Prefill up to min_prompt_len tokens, then treat other prefill as\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[38;5;66;03m# decode and ignore output.\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_seq_len - min_prompt_len):\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m   next_token_ids, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_token_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_token_ids_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_positions_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_mask_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_positions_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_ps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_ps_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_ks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_ks_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_local_mask_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m   curr_prompt_mask = prompt_mask_tensor.index_select(\n\u001b[32m    592\u001b[39m             \u001b[32m1\u001b[39m, output_index).squeeze(dim=\u001b[32m1\u001b[39m)\n\u001b[32m    593\u001b[39m   curr_token_ids = token_ids_tensor.index_select(\n\u001b[32m    594\u001b[39m             \u001b[32m1\u001b[39m, output_index).squeeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/v09_tr_tokenizer/gemma_model.py:464\u001b[39m, in \u001b[36mGemmaForCausalLM.forward\u001b[39m\u001b[34m(self, input_token_ids, input_positions, kv_write_indices, kv_caches, mask, output_positions, temperatures, top_ps, top_ks, local_mask, **kwargs)\u001b[39m\n\u001b[32m    461\u001b[39m normalizer = torch.tensor(\u001b[38;5;28mself\u001b[39m.config.hidden_size**\u001b[32m0.5\u001b[39m, dtype=hidden_states.dtype, device=hidden_states.device)\n\u001b[32m    462\u001b[39m hidden_states = hidden_states * normalizer\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m embedder_weight = \u001b[38;5;28mself\u001b[39m.embedder.weight\n\u001b[32m    473\u001b[39m hidden_states = hidden_states.index_select(\n\u001b[32m    474\u001b[39m \u001b[32m1\u001b[39m, output_positions).squeeze(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/tokenizer/v09_tr_tokenizer/gemma_model.py:378\u001b[39m, in \u001b[36mGemmaModel.forward\u001b[39m\u001b[34m(self, hidden_states, freqs_cis, kv_write_indices, kv_caches, mask, local_mask)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers)):\n\u001b[32m    373\u001b[39m     layer = \u001b[38;5;28mself\u001b[39m.layers[i]\n\u001b[32m    374\u001b[39m     hidden_states = layer(\n\u001b[32m    375\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    376\u001b[39m         freqs_cis=freqs_cis.get(layer.attn_type),\n\u001b[32m    377\u001b[39m         kv_write_indices=kv_write_indices,\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         kv_cache=\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m    379\u001b[39m         mask=mask,\n\u001b[32m    380\u001b[39m         local_mask=local_mask,\n\u001b[32m    381\u001b[39m     )\n\u001b[32m    382\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "gemma_model.generate(\"Merhaba, nasıl yardımcı olabilirim?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
